<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link href="https://fonts.googleapis.com/css?family=Maven+Pro:400,500&amp;subset=latin-ext,vietnamese" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Dancing+Script:400,700&amp;subset=vietnamese" rel="stylesheet">
  <meta name="google-site-verification" content="8zqeFQNuNAWS7ye6oN69hdEeYC_RsDyAlhht79xtAQo" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="/build-notes/assets/img/banner.png" />

  

  <title>
    
      Search on this page | BUILD Notes
    
  </title>

  

  <!-- page's cover -->
  
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1234">
    <meta property="og:image:height" content="592">
  

  

  <link rel="shortcut icon" type="image/x-icon" href="/build-notes/assets/img/favicon.ico">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/css/materialize.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="/build-notes/assets/css/main.css">
  <link rel="stylesheet" href="/build-notes/assets/css/thi_scss.css">
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;400;600&display=swap" rel="stylesheet"> 

  
    
      <link rel="stylesheet" href="/build-notes/assets/css/page.css">
    
  

  
    
      <link rel="stylesheet" href="/build-notes/assets/css/page.css">
    
  
    
      <link rel="stylesheet" href="/build-notes/assets/css/search.css">
    
  

  <link rel="stylesheet" href="/build-notes/assets/css/syntax.css">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/build-notes/feed.xml">
  <link rel="sitemap" type="application/xml" title="Sitemap" href="/build-notes/sitemap.xml">
  
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Search on this page" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I want to write it down, and for it to be pretty." />
<meta property="og:description" content="I want to write it down, and for it to be pretty." />
<link rel="canonical" href="/build-notes/search/" />
<meta property="og:url" content="/build-notes/search/" />
<meta property="og:site_name" content="BUILD Notes" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search on this page" />
<script type="application/ld+json">
{"url":"/build-notes/search/","@type":"WebPage","description":"I want to write it down, and for it to be pretty.","headline":"Search on this page","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>
	<header>
  
    <div class="container">
  <a href="#" data-activates="slide-out" class="button-collapse top-nav full hide-on-large-only">
    <i class="material-icons">menu</i>
  </a>
</div>
<ul id="slide-out" class="side-nav fixed">
  <li>
    <div class="userView thi-userView">
      <div class="background"></div>
        <a href="/build-notes/">
          <img style="display:inherit;" class="circle z-depth-2" src="/build-notes/assets/img/napkins.png">
        </a>
      <span style="font-size: larger;" class="white-text name">BUILD Notes</span>
      <span class="white-text email"><a style="color: white; font-weight:400" href="https://github.com/buildsi/build-notes">buildsi/build-notes</a></span>
    </div>
  </li>
  <li style="padding: 10px;">
    <form action="/build-notes/search" method="get">
      <input class="search-sidebar" type="search" name="q"  placeholder="Search something?" autofocus>
      <input type="submit" value="Search" style="display: none;">
    </form>
  </li>
  <div id="nav-bar">
    <li><a class="waves-effect" href="/build-notes/">Home</a></li>
    <li><div class="divider"></div></li>
    <li><a class="waves-effect" href="/build-notes/papers/">Papers</a></li>
    <li><div class="divider"></div></li>
    <li><a class="waves-effect" href="/build-notes/terms/">Terms</a></li>
    <li><div class="divider"></div></li>
    <li><a class="waves-effect" href="/build-notes/about/">About</a></li>
    <li><div class="divider"></div></li>
    <li><a class="waves-effect" href="/build-notes/references/">References</a></li>
    <li><div class="divider"></div></li>
    <li><a class="waves-effect" href="/build-notes/getting-started/">Getting Started</a></li>
  </div>
</ul>

  
</header>
<main>
	<div class="container">
	  <div id="page-info">
		  <h2>Search on this page</h2>
		</div>
		<div class="row">
			<form action="/build-notes/search" method="get">
	<input type="search" name="q"  placeholder="Search for..." autofocus>
	<input type="submit" value="Search" style="display: none;">
</form>


<p style="opacity: 0.6; color: darkmagenta; font-size: 1.2rem;margin-bottom: 20px;" ><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for keyword "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					
					"how-to-write-notes": {
						"id": "how-to-write-notes",
						"title": "How to write notes",
						"categories": "notes-jekyll",
						"url": " /how-to-write-notes",
						"content": "tocIn this post\n  \n  \n\n  Front Matter\n  Code    \n      Syntax highlighting\n      Keyboard\n    \n  \n  Insert a figure    \n      Standard\n      Inline\n    \n  \n  Insert an video from Youtube\n  For writing posts    \n      Remove heading numbering\n      Manually assign an id for a heading\n      Insert toc (Table of Contents)\n      Make a list into columns\n      Insert a read-more link\n      Insert steps\n    \n  \n  Mathematical expressions    \n      Inline math\n      Block        \n          Special Characters\n          Matrices\n          Labels\n        \n      \n    \n  \n  Boxes    \n      Theorem boxes\n      Interative coding blocks\n      Notification boxes\n      Insert Accordian\n      Insert blockquote\n      Summary block\n    \n  \n  Categories and tags    \n      Use it\n      Create a new category\n      Already-defined tags / categories\n    \n  \n  Text\n  Figures / Tables / Videos\n  Boxes    \n      Hide/Show\n      Notifications\n      Pull quotes\n      Summarization box\n      Theorem style\n      Important boxes\n    \n  \n  Others    \n      Side by side\n      Links\n      Notices\n      Steps\n    \n  \n\n\n  \n\n\nFront Matter\n\nFront matter refers to the section of text at the top of a posts file.\nFor example:\n\n---\nlayout: post\ntitle: How to use NoteTheme?\n---\n\n\nThe following components are included:\n\n\n  maths: 1 add this if you want to use mathematical expression in the post.\n  toc: 1 if you want to display a table of contents on the left sidebar\n  terms: 1 if you want the sidebar to show a list of site terms\n  datacamp: 1 if you want to embed a datacamp interactive code question (see below)\n  categories: [notetheme,jekyll] add category you want\n  tags: [notetheme,jekyll,code] add tag you want\n  date: 2018-08-21 if you update the post, write the updated date\n\n\nCode\n\nSyntax highlighting\n\nYou can insert any block of code you want with a syntax highlight effect like below\n\nAvailable languages : ruby, python, c, matlab, latex, html, css, javascript.\n\n{% highlight ruby %}\n\n{% endhighlight %}\n\n\nFor example, Python with line numbering,\n\n{% highlight python linenos %}\n\n{% endhighlight %}\n\n\nor something appears in the command line window like this\n\n  $ sudo apt-get update\n\nby using\n&lt;div class=\"terminal\" markdown=\"1\"&gt;\n`$ sudo apt-get update`\n&lt;/div&gt;\n\n\nKeyboard\n\nYou can also insert a keyboard key like this Ctrl + B, just use &lt;kbd&gt;Ctrl&lt;/kbd&gt;.\n\nInsert a figure\n\nStandard\n\n![](/link/to/figure/){:.w-500 .no-border}\n\n\n\n  \n    no-border: remove the border around figure\n    w-300: reduce the size of image to maximum 300px (if the screen’s maximum size is 500px, the figure’s size will be 100% the size of the screen). You can change the number 3 in 300 by other numbers 2,4,5,6,7,8,10 or something like w-150.\n  \n\n\nInline\n\n{% include img-inline.html content=\"/link/to/figure/\" %}\n\n\nInsert an video from Youtube\n\nDetermine the youtube video’s url, like this\n\nhttps://www.youtube.com/watch?v=wIsK4kQTrIg\n\n\nChoose wIsK4kQTrIg and put it inside below code\n\n{% include youtube.html content=\"wIsK4kQTrIg\" size=\"5\" %}\n\n\nIn that, 5 represents the percent your youtube container width in comparison with the page’s width, for example, 5 means “50%”. You can choose any integer number between 3 and 10. This size is only available on a wide screen (min width is 993px), when the screen is smaller than 993px, the width becomes automatically 100% of page width.\n\nFor writing posts\n\nRemove heading numbering\n\nIf you don’t want display number before some heading, just put BEFORE this heading {.nocount}, for example,\n\n{:.nocount}\n## Heading without numbering\n\n\nIf you want to use multiple classes, separate them by space.\n\nManually assign an id for a heading\n\nIndert {:#your-id} before this heading, for instance,\n\n{:#your-id}\n## Heading\n\n\nIf you want to then use it\n\n[Caption]({{ post.url }}#your-id)\n\n\nInsert toc (Table of Contents)\n\nThere are actually two choices for the Table of contents! You can add one manually (as\nyou see to the right, here):\n\n{% include toc.html %}\n\n\nAnd you can also see the one generated in the left sidebar via the front end matter:\n\ntoc: 1\n\n\nMake a list into columns\n\nPut the list inside a &lt;div&gt; tag like,\n\n&lt;div class=\"thi-columns\" markdown=\"1\"&gt;\n- item 1\n- item 2\n- item 3\n- item 4\n- item 5\n- item 6\n&lt;/div&gt;\n\nAnd the result looks like this - notice how we span two colums!\n\n\n  \n    item 1\n    item 2\n    item 3\n    item 4\n    item 5\n    item 6\n  \n\n\nInsert a read-more link\n\n{% include more.html content=\"[Welcome to Math2IT](http://math2it.com).\" %}\n\n\n\n\tkeyboard_arrow_right\n\tWelcome to Math2IT.\n\n\nInsert steps\n\nIf you want something like this:\n\n\n\n\n\n\n      Content in step 1.\n    \n\n\n\n\n\n\n      Content in step 2\n    \n\n\n\nDo the following:\n\n&lt;div  class=\"thi-step\"&gt;\n&lt;div class=\"step\"&gt;\n&lt;div class=\"step-number\"&gt;&lt;/div&gt;\n&lt;div class=\"step-content\" markdown=\"1\"&gt;\nContent in step 1.\n&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div class=\"step\"&gt;\n&lt;div class=\"step-number\"&gt;&lt;/div&gt;\n&lt;div class=\"step-content\" markdown=\"1\"&gt;\nContent in step 2\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n\n\nMathematical expressions\n\nInline math\nFor inline math, use $math-expression$\n\nBlock\nFor an entire block, you can use $$math block$$ or\n\n~~~ latex\n$$\nx^n + y^n = z^n\n$$\n~~~\n\n\nSpecial Characters\nIf you want to insert some special characters, you must put \\ before this character, for instance, \\\\{ 1,2,3 \\\\} gives $\\{ 1m2,3 \\}$\n\nTips\n\n  If you type inline math that contains characters _, you must add \\ before each of them, for example, a\\_1 give $a_1$.\n  Don’t use || for absolute values, let’s use \\vert \\vert instead.\n  Don’t use \\left\\| \\right\\| for norms, use \\Vert \\Vert instead.\n  Don’t use * for star symbols, use \\ast instead.\n  If you want to type \\\\, type \\\\\\\\ instead.\n\n\nMatrices\nIf you want to type an inline matrix, e.g., $[A]=\\begin{bmatrix}1 &amp; 2 \\\\ 2 &amp; 3.999 \\end{bmatrix}$, type like below,\n\n~~~ latex\n$[A]=\\begin{bmatrix}1 &amp; 2 \\\\\\\\ 2 &amp; 3.999 \\end{bmatrix},$\n~~~\n\n\nLabels\nIn order to use \\label{} and \\eqref{} like in latex, use\n\n~~~ latex\n$$\n\\begin{align}\\tag{1}\\label{eq1}\nx^n + y^n = z^n\n\\end{align}\n$$\n\nCall again equation $\\eqref{eq1}$.\n~~~\n\n\nThe above renders to:\n\n$$\n\\begin{align}\\tag{1}\\label{eq1}\nx^n + y^n = z^n\n\\end{align}\n$$\n\nCall again equation $\\eqref{eq1}$.\n\n\nYou don’t need an enviroment align or equation to use \\label, you can use it with $$ only, for example,\n\n~~~ latex\n$$\nx^n + y^n = z^n \\tag{1}\\label{eq1}\n$$\n\nCall again equation $\\eqref{eq1}$.\n~~~\n\n\nBoxes\n\nTheorem boxes\n\nA theorum box is a nice little callout to highlight some special text. You\ncan create one as follows:\n\n&lt;div class=\"thi-box\" markdown=\"1\"&gt;\n&lt;div class=\"box-title\" markdown=\"1\"&gt;\n**Title**\n&lt;/div&gt;\n&lt;div class=\"box-content\" markdown=\"1\"&gt;\nContent\n&lt;/div&gt;\n&lt;/div&gt;\n\nwhich renders into:\n\n\n  \n    Title\n  \n  \n    Content\n  \n\n\nInterative coding blocks\n\nYou even can embed R/Python code environment inside a post like this.\n\n\n\t\n\t\t# This will get executed each time the exercise gets initialized\n\t\tb = 6\n\t\n\t\n\t\t# Create a variable a, equal to 5\n\t\t# Print out a\n\t\n\t\n\t\t# Create a variable a, equal to 5\n\t\ta &lt;- 5\n\n\t\t# Print out a\n\t\tprint(a)\n\t\n\t\n\t\ttest_object(\"a\")\n\t\ttest_function(\"print\")\n\t\tsuccess_msg(\"Great job!\")\n\t\n\tUse the assignment operator (&lt;-) to create the variable a.\n\n\nNotification boxes\n\nA notification box is useful to draw the eyes of the reader to important content.\nYou could create a warning, for example, like this:\n\n{% include warning.html content=\"Warning's content\" %}\n\n\nRendered it looks like the following:\n\nerrorWarning’s content\n\nThere is another style for information:\n\n{% include tip.html content=\"Info's content\" %}\n\n\nwhich renders to:\n\ninfoInfo’s content\n\nInsert Accordian\n\nAn accordian menu hides some content, and the user can click to reveal it. It’s\nbest to use an icon on it that suggests that the user should click, like an arrow.\n\nUse these lines of code\n\n&lt;ul class=\"collapsible\" data-collapsible=\"accordion\"&gt;\n&lt;li&gt;\n&lt;div class=\"collapsible-header\" markdown=\"1\"&gt;&lt;i class=\"material-icons\"&gt;expand_more&lt;/i&gt;\nTitle\n&lt;/div&gt;\n&lt;div class=\"collapsible-body\" markdown=\"1\"&gt;\nContent\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n\n\nThe above renders into an expandable section:\n\n\n\n\n      expand_more\nTitle\n    \n\n      Content\n    \n\n\n\nInsert blockquote\n\nA blockquote is a nice style to show a quote, for example:\n\n\nThe content of extra info of the post.\n\n\nAnd you can generate the above as follows:\n\n&lt;p class=\"post-more-info\" markdown=\"1\"&gt;\nThe content of extra info of the post.\n&lt;/p&gt;\n\n\n  Other normal blockquote like this.\n\n\nSummary block\n\nA summary block has a title, and then some content, and is another strategy\nto point out more salient information. If you use these lines of code:\n\n&lt;fieldset class=\"field-set\" markdown=\"1\"&gt;\n&lt;legend class=\"leg-title\"&gt;Title&lt;/legend&gt;\nContent\n&lt;/fieldset&gt;\n\nYou can then generate:\n\n\n  Title\n  Content\n\n\nCategories and tags\n\nUse it\n\nOn the front matter of each post, use this\n\ncategories: [maths, python] \ntags: [algebra, function, theorem]\n\n\nCreate a new category\n\n\n  Below the title of each post, you will see “in \", for example, this post **in NoteTheme**.\n  Open file _data\\categories.yml and add the new category you want\n    \n      slug: the id of this category, it will appear in the url, like\n          https://dinhanhthi.github.io/notetheme/categories#notetheme\n        \n      \n      name : the name of this catefory, it will appear on the site, like notetheme\n    \n  \n\n\nAlready-defined tags / categories\n\nAlready-defined categories:\n\n\n\n\nregistration : registration\n\nnotes-jekyll : notes-jekyll\n\n\n\n\n\nAlready-defined tags: \n\njekyll\n\n.\n\n\n\n\nText\n\n\n  New badge: &lt;new /&gt;\n  Update bagge: &lt;update /&gt;\n  Keyboard: &lt;kbd&gt;Ctrl&lt;/kbd&gt;\n\n\n\n\n\n\nCtrl\n\nFigures / Tables / Videos\n\n\n  \n    Normal way\n\n    ![](/link/to/figure/){:.w-500 .no-border}\n    \n  \n  \n    Inline figures\n\n    {% include img-inline.html content=\"/link/to/figure/\" %}\n    \n  \n  \n    Youtube video\n\n    {% include youtube.html content=\"wIsK4kQTrIg\" size=\"5\" %}\n    \n  \n\n\nBoxes\n\nHide/Show\n\n&lt;ul class=\"collapsible\" data-collapsible=\"accordion\"&gt;\n&lt;li&gt;\n&lt;div class=\"collapsible-header\" markdown=\"1\"&gt;&lt;i class=\"material-icons\"&gt;face&lt;/i&gt;\nTitle\n&lt;/div&gt;\n&lt;div class=\"collapsible-body\" markdown=\"1\"&gt;\nContent\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n\n\nNotifications\n\n\n  \n    Info\n\n    &lt;p markdown=\"1\" class=\"thi-tip\"&gt;\n&lt;i class=\"material-icons mat-icon\"&gt;info&lt;/i&gt;\ncontent\n&lt;/p&gt;\n    \n  \n  \n    Error\n\n    &lt;p markdown=\"1\" class=\"thi-warning\"&gt;\n&lt;i class=\"material-icons mat-icon\"&gt;error&lt;/i&gt;\ncontent\n&lt;/p&gt;\n    \n  \n\n\nPull quotes\n\n&lt;p class=\"post-more-info\" markdown=\"1\"&gt;\nThe content of extra info of the post.\n&lt;/p&gt;\n\n\nSummarization box\n\n&lt;fieldset class=\"field-set\" markdown=\"1\"&gt;\n&lt;legend class=\"leg-title\"&gt;Title&lt;/legend&gt;\nContent\n&lt;/fieldset&gt;\n\n\nTheorem style\n\n&lt;div class=\"thi-box\" markdown=\"1\"&gt;\n&lt;div class=\"box-title\" markdown=\"1\"&gt;\n**Title**\n&lt;/div&gt;\n&lt;div class=\"box-content\" markdown=\"1\"&gt;\nContent\n&lt;/div&gt;\n&lt;/div&gt;\n\n\nImportant boxes\n\n&lt;div class=\"p-mark\" markdown=\"1\"&gt;\nContent\n&lt;/div&gt;\n\n\nOthers\n\nSide by side\n\n&lt;div class=\"row d-flex\" markdown=\"1\"&gt;\n&lt;div class=\"col s12 l6\" markdown=\"1\"&gt;\nThis is the code\n&lt;/div&gt;\n&lt;div class=\"col s12 l6\" markdown=\"1\"&gt;\nThis is the result\n&lt;/div&gt;\n&lt;/div&gt;\n\n\nLinks\n\nSee again\n\n&lt;div class=\"see-again\"&gt;\n&lt;i class=\"material-icons\"&gt;settings_backup_restore&lt;/i&gt;\n&lt;span markdown=\"1\"&gt;\nContent\n&lt;/span&gt;\n&lt;/div&gt;\n\n\nRead-more link\n\n{% include more.html content=\"[Welcome to Math2IT](http://math2it.com).\" %}\n\n\nDownload\n\n{% include download.html content=\"[Download text](download link).\" %}\n\n\nNotices\n\nYou can create a notice as follows:\n\n&lt;fieldset class=\"field-set\" markdown=\"1\"&gt;\n&lt;legend class=\"leg-title\"&gt;TL;DR&lt;/legend&gt;\n- Show the post in a flexible way.\n- Show the figures any place\n- Show the maths, the code blocks in a beautiful way.\n- and many things else...\n&lt;/fieldset&gt;\n\n\nAnd it looks like this\n\n\n  TL;DR\n  \n    Show the post in a flexible way.\n    Show the figures any place\n    Show the maths, the code blocks in a beautiful way.\n    and many things else…\n  \n\n\nSteps\n\n&lt;div  class=\"thi-step\"&gt;\n&lt;div class=\"step\"&gt;\n&lt;div class=\"step-number\"&gt;&lt;/div&gt;\n&lt;div class=\"step-content\" markdown=\"1\"&gt;\nContent in step 1.\n&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div class=\"step\"&gt;\n&lt;div class=\"step-number\"&gt;&lt;/div&gt;\n&lt;div class=\"step-content\" markdown=\"1\"&gt;\nContent in step 2\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;"
					}
					
				
			
		
		
			
				
					,
					
					"papers-abate-ocaml14-pkg-manager-prefs": {
						"id": "papers-abate-ocaml14-pkg-manager-prefs",
						"title": "Abate+ocaml14 Pkg Manager Prefs",
						"categories": "",
						"url": " /papers/abate+ocaml14-pkg-manager-prefs",
						"content": "Using Preferences to Tame your Package Manager\n\nOverview\n\nThis paper appears to be for a talk, and starts by explaining why we need package managers, \nprimarily because considering all combinations (e.g., of versions) is NP-complete. It then \nintroduces the idea of using solvers that can honor user preferences to come up with a solution, \nand how it should be possible to honor user preferences. E.g.:\n\n\n  “I want to minimize the number of packages I need to upgrade.”\n\n\nand specifically highlights the opam package manager, \nwhich uses CUDF pivot format to represent these user preferences. \nAn interesting idea is that the solver is modular, but doesn’t necessarily need to be alongside the software. They highlight\nmaking an external “solver farm” available (in the cloud) at their institution (Iril) for this purpose.\n\nTakeaways\n\nThis talk might have introduced a novel idea of not just using modular solvers, but having them\nbe based in the cloud so an entire community can use them from anywhere.\nThe authors also seem to be suggesting that it’s best practice to use some kind of standard that also allows\nfor user preferences (the one developed by the Mancoosi project) and then coming up with a solution (in CUDF format) that comes down to a list of changes to make on the system.\n\nTerms\n\n\n  OCaml is a programming langufage developed at Inria.\n  OPAM is a package manager for OCaml\n\n\nMancoosi Project\n\nThe Mancoosi project was created to generally develop FOSS tools\nfor sysadmins (e.g., package managers, and standards). This paper is concerned with it’s preference\nlanguage, which describes using package selectors and measurement functions to control\nhow dependency resolution is done by the manager.\n\nPackage Selectors\n\nThese identify a set of packages, e.g., solution, new, removed, changed, up, and down. For example,\n“changed” would be the set of packages that were changed in some proposed solution, S.\n\nMeasurements\n\nAn integer valued function that can be applied to any package selector, plus additional arguments. For\nexample, I could do count(X) to count the number of entries in some set X. You should reference the second\npage of the PDF to see all the different measurement functions.\n\nCUDF\n\nSee the [cudf]\n term page for notes from this paper."
					}
					
				
			
		
			
				
					,
					
					"papers-abate-2013-modular-package-manager": {
						"id": "papers-abate-2013-modular-package-manager",
						"title": "Abate 2013 Modular Package Manager",
						"categories": "",
						"url": " /papers/abate-2013-modular-package-manager",
						"content": "A Modular Package Manager Architecture\n\nTLDR\n\nMancoosi Package Manager (MPM) is a hodge podge of debian tools (the client, and hooks to apt) in Python and standards\n(using CUDF as input output for an upgrade scenario) and it’s slow as heck, but performs better than other\nDebian-based package mangers. This paper is more a review of CUDF and then argument that modularity is the way\nto go, as demonstrated by a few studies that show this strategy comes up with more optimal solutions.\n\nOverview\n\nThe authors present package managers the tools to install/remote software, and argue\nthat a modular architecture is ideal to allow for choosing backends and dependency solvers.\nInstalling packages is hard because:\n\n\n  you can’t easily put them together, they are modular\n  you can’t easily install more than one version of the same thing\n  they can change rapidly, and an initial install of several packages can break in the future (conflits)\n  you have to often install them on shared resources with different levels of permissions, etc.\n\n\nBecause installing packages is hard, we have package managers! Package managers generally:\n\n\n  they are the tools that the user interacts with (e.g., “apt-get install…)\n  can get and install packages from some remote resource (e.g., mirrors, cloudy places, GitHub)\n  they have some means to assess dependencies and create an update plan (e.g., that CUDF text file)\n  they execute the update plan, and fall back if things fail.\n\n\nThis can get complicated very quickly because of logical complexity (if you have a huge set of dependencies\nand conflicts it’s harder to find a solution) and scale (the more packages, the bigger the hairball to untangle).\nScale also takes into account the size of “the place you install packages from.” The more you have,\nthe more resources that are needed for the remote, period.\n\nThe authors argue that the front end (the client interacting with the user) should be separate from the\nback end (the dependency solver). They present their proof of concept, the Mancoosi Package Manager \n(MPM) that uses the Common Upgradeability Description Format (CUDF) and something called \n“the user preferences language” that helps do just that -\nget preferences from the user.\n\nThe Upgrade Problem\n\nYou are usually okay when starting with a FOSS operating system, but as soon as a few packages\nupgrade, you start running into dependency problems. This leads us to… dependency hell!\n\n\n  the user gets entangled in an inextricable web of dependencies and conflicts that state-of-the-art package managers are unable  to handle.\n\n\nAnd I love that this definition is included in the paper! They next walk through a dummy\nexample of how things break with python-simpy, showing the packages updated, installed, and removed for each \n(see [1]). They show how MPM is better because it gives the user\ncontrol to specify criteria. For example:\n\nmpm -c \"-removed,-changed\" install python-simpy\n\n\nSays to install python-simply and minimize removed and changed packages. The command is slow (~10 seconds)\nand they justify this by saying it’s just a proof of concept. This gives us a good idea for a framework to test package managers.\n\ninfoWe would want to be able to build contains across package managers (apt-get, yum, aptitude, etc.) and then simply install a particular package, and parse the terminal output to see the decision that was decided upon.\n\nSee the containers idea section for this idea.\n\nModular package management\n\nThe authors say that dependency solving is really hard, and it makes sense to de-couple it from\nthe rest of the package manager (the client to manage software components) so that they can be re-usable \ncomponents. This brings back the idea of “solver farms,” or externally hosted solvers. \nFigure 1 in their paper shows “modular package manager architecture” but I didn’t find it very intuitive.\n\nScenarios\n\nThis is the first paper that I’ve encountered that describes different scenarios that package\nmanagers can run into (e.g., healthy, peaceful). This is something that I think should be better flushed\nout into an idea - “What is the standard or schema for some package manager flow?”\nSee the flow page for this idea.\nIt sounds like there is work in progress to standardized criteria and aggregation functions,\nbut this paper was 7 years ago, so we will need to read a more recent one (abate 2020?)\nto know the current status.\n\nSolvers\n\nThis paper has a short list of solvers (p. 16) that use CUDF, but I remember a more robust table\nin the Abate 2020 paper, so I won’t reproduce the list here.\n\nTakeaways\n\nThe takeaway from this paper is that many package managers use ad-hoc heuristics, and this is messy.\nIf we uncouple dependency resolution from the package manager (e.g., the solvers) we can\nmore easily share components. The package manager should act as a skeleton client to interact with the\nuser, and we should be able to plug in components. This paper presents the general idea of embracing\nalready existing package managers (e.g., apt) and functionality, and integrating into a new tool (MPM)\nthat can take as input criteria and solvers, and then use CUDF as an input/output format to make \nthem more modular. I think this was probably a novel idea back in 2013, but now there must be more\nrecent work that improves upon the ideas.\n\nTerms\n\n\n  package A software component paired with metadata that describes how to install it on a system. Each package can follow a unique development pipeline and even versioning scheme reference.\n  CUDF The Common Upgradeability Description Format, a text file that describes all conflicts, and dependencies based on some user preferences to install software. See the [cudf]\n term page for notes from this paper.\n  user preference language Allows to specify a list of criteria (sometimes called measurements) of package selectors, and each has a plus or minus prefix to indicate minimizing or maximizing.\n\n\nMancoosi Package Manager (MPM)\n\nThis is a proof of concept implementation of the idea of the authors, a modular package manager.\nIt uses CUDF to capture dependency information and the user preference langauage to ask the user\nfor what to optimize, etc. The implication is that the MPM is a creation of the Mancoosi project.\nThe idea of MPM is that it uses CUDF as input/output. MPM:\n\n\n  uses apt on the back end to parse the command line and install packages\n  has removed, new, changed, and notuptodate as criteria\n  you can specify a solver plugin with -s &lt;solver&gt; (that’t kind of cool! Will we try this with a custom solver?)\n  you can specify a criterion with -c &lt;option&gt; (e.g., -removed)\n  -o &lt;option&gt; passes an option to apt\n  debtodudf is used to translate Debian package metadata to CUDF\n  MPM is written in Python and uses Python bindings to apt (wow, didn’t expect that!)\n  supports all the same command line arguments as apt\n  uses the aspcud solver that won one of the MISC competitions.\n\n\ninfoRegardless of the format we use to represent an upgrade scenario (e.g., CUDF) there would always need to be some kind of translator to derive it from the original user request and package manager.\n\nThe above has me wondering if we want to just work on the solver (e.g., take the same constraints but\nadd another level of information to it) or if we want to require the CUDF specification to better represent\nsome of this information (arguably this would be harder because the package managers would have to extract it,\nand might be redundant if we would need to double check again). I’m also wondering why there is such a focus on\napt/Debian.\n\nPerformance Testing\n\nThe authors first justify that time isn’t important because their implementation is a proof of concept.\nFor testing they use containers, and do 5 groups of tests that add/remove the same package under\ndifferent Debian releases. The salient difference in the package release universes is that newer ones\nhave many more packages (logically). For each, they did 162 (why 162?) install/remove requests, and randomly\nselected packages ensuring that there is a solution (again, how?) They only consider results that made it\nwithin the 300s timeout. They classified results into three categories:\n\n\n  best solution\n  optimal solution\n  failure (crashed or not a correct solution)\n\n\nThey show that performance varies between 50% and 75%, with MPM being the best (see figure).\n\n\n\nThis is overall one way to go about testing how well solvers work between different package managers, but it\nseems rough around the edges because they are working as black boxes for the most part.\n\nRelated Work\n\nPages 20-21 walk through Debian and RHEL distribution package managers (apt vs. yum) and all the\nassociated tools that do everything from fetch packages to using solvers. The interesting observation\nhere is that tools are optimized to their repository sets, e.g., a well defined set of repository metadata\ncan allow for a heuristic that ensures some solution.  Table 2 is useful (and small enough to include here)\n\n\n  \n    \n      Tool\n      Solver\n      Optimization\n      Complete\n    \n  \n  \n    \n      apt-get\n      internal\n      hard-coded\n      no\n    \n    \n      aptitude\n      internal\n      hard-coded\n      no\n    \n    \n      cupt\n      internal\n      hard-coded\n      no\n    \n    \n      smart\n      internal\n      programmable\n      maybe\n    \n  \n\n\nWow, there are at ton of “hard-coded”s in that table!\n\nData and Appendix\n\n\n  Appendix A has a nice summary of the CUDF format\n  Data for package install comparison analysis\n  Data Writeup with plot."
					}
					
				
			
		
			
				
					,
					
					"papers-abate2012dependency": {
						"id": "papers-abate2012dependency",
						"title": "Abate2012dependency",
						"categories": "",
						"url": " /papers/abate2012dependency",
						"content": "Dependency Solving: a Separate Concernin Component Evolution Management\n\nNote that this paper seems very similar to [1], although the date is one year earlier. I’m going to do a more superficial reading to look for new information. It would be interesting to have more context about why there are these two similar papers\n\nTLDR\n\nThis paper [2] is indeed similar to [1] - the authors want to establish dependency solving as a “separate concern” by using CUDF and rationalize this by showing how well it works with implementations from the MISC competition.\nThey show that complexity comes down to:\n\n\n  conflicts (metadata or different versions of the same thing),\n  large number of software packages in the repository, and\n  complexity of user preferences given 1 and 2.\n\n\nOverview\n\nThe paper has a nice introduction with “the law of continuing change”\n\n\n  A program that is used and that as an implementation of its specification reflectssome other reality, undergoes continual change or becomes progressively less useful.\n\n\nAnd this seems similar to the upgrade problem from [1]. Basically, the more compoenents with different dependencies, the harder the problem, and the greater importance of tools to help. They note that “software modules” have different names depenending on the environment (e.g., some things have extensions, plugins, bundles, add-ons, etc.)\n\nThe Upgrade Problem\n\nThe authors again describe the upgrade problem.\n\nComplexity of the Upgrade Problem\n\n\n  given any request for install/remove, there are an exponential number of solutions (NP-complete)\n  there could actually be this many, and how do we choose the best?\n\n\nPackage metadata is used to figure out solutions, including:\n\n\n  name\n  version\n  conflicts: “Don’t install this alongside!”\n  features: “What is provided by installing this?”\n\n\nProperties of a repository of components:\n\n\n  abundance: every package in the repository has dependencies satsfied by other packages in the repository (like a little, closed ecosystem that doesn’t need anything external)\n  peace: no package in the repository conflicts with another package in the repository.\n\n\nThe authors then present theorums (and proofs!) for the following (these are again verbatim):\n\nTheorem 1.\n\n\n  Satisfiability of package upgrade requests is NP-complete, provided the com-ponent model features conflicts and disjunctive dependencies.\n\n\nTheorem 2.\n\nInstallability of a package in an empty environment is in PTIME in any of the two following cases:\n\n\n  The component model does not allow for conflicts.\n  The component model does not allow for disjunctive dependencies or features, and the repository does not contain multiple versions of packages.\n\n\nComplexity in the Case of Component Evolution\n\nInstalling new packages gets more complicated in that you have to upgrade old packages.\nDifferent managers handle this differently:\n\n\n  Debian only allows installing one version of a package at a time\n  RPM allows installing multiple at once, but it won’t work if packages with the same version have the same filesyste path\n\n\nflat means that two packages can’t be installed with the same name.\n\nTheorem 3.\n\n\n  Existence of a flat installation containing a component is NP-complete, even when the component model does not allow for explicit conflicts, alternatives, and features.\n\n\nDealing with exponentially many solutions\n\nThis is the step where we evaluate solutions as “paranoid” or “trendy” or otherwise\ntry to satisfy user preferences.\n\nDependency solving as a separate concern\n\nThe upgrade problem needs to be represented in a structured way in order to be able to “treat dependency solving as a separate concern.” We need to know:\n\n\n  describe all known installed and available components\n  describe components that need to be installed, removed, upgraded with constraints\n  what the user wants (user preferences)\n\n\nThey again present CUDF, a domain specific language to represent points 1 and 2. CUDF has:\n\n\n  platform independence: agnostic to package manager, version scheme, etc.\n  solver independence\n  readability: it’s a simple text file\n  extensibility: only essential properties are captured in CUDF, but you can declare and use others\n  formal semantics: e.g., the structured terminology\n\n\nTheir CUDF section here is exactly the same as in [1], I guess it’s okay to plagarize yourself :)\n\nMancoosi International Solver Competition\nThe authors discuss specifics of MISC 2010, generally, participant solvers received a CUDF document as input and needed to produce a CUDF encoded solution. There were problems ranging from easy to difficult, and participants could choose a paranoid or trendy scenario.\n\n\n\nAnd there are complete results of the competition on page 17:\n\n\n\nIt’s interesting that the solvers perform differently depending on the problem sets.\nThe “trendy” scenario is always more complex because there are more parameters.\nFigure 6. has a chart that compares package managers, but it uses patterns for bars\nand is incredibly hard to read. It looks like unsa is the best, consistently finding the\nbest solution and being fastest.\n\nData and Appendix\n\n\n  Appendix A and B have nice summaries of CUDF"
					}
					
				
			
		
			
				
					,
					
					"papers-abate2020dependency": {
						"id": "papers-abate2020dependency",
						"title": "Abate2020dependency",
						"categories": "",
						"url": " /papers/abate2020dependency",
						"content": "Dependency Solving Is Still Hard,but We Are Getting Better at It\n\nTLDR\n\nThe main theme of the paper is to (in the author’s words):\n\n\n  treat dependency solving as a separate concern in package manager design and implementation, delegating it to a specialized, highly-capable dependency solver based on  state-of-the-art constraint solving and opti-mization techniques\n\n\nBasically, dependency solving is (still) a hard (NP-complete) problem, and the authors take\nissue that solver implementations should not have random, ad-hoc heuristics, and instead should use “tried and tested” techniques that can be plugged in to any package manager like plugins. This paper\nthus reviews several of these solving techniques (SAT-solving, PBO, and MILP).\nThey think SAT solvers have the most promise.\n\nOverview\n\nWhat is a package manager responsible for?\n\nThis paper nicely describes the duties of a package manager:\n\n\n  Take as input:\n    \n      the current status of packages on the system\n      a universe of all available packages (in other papers called U)\n      a user request (e.g., “install xyz, remove xyz”)\n      user preferences (e.g., “minimize updated software”)\n    \n  \n  Return as output:\n    \n      an upgrade plan: a list of actions to take on the system to reach the status that the user wants (e.g., installing thething)\n    \n  \n\n\nThe user has expectations that, even without specifying preferences, they will\nget the minimal set of packages needed to run the software that they want (imagine\ntrying to install something, it reports success, but then getting an error that a dependencyis missing!)\n\nSee the package managers section for how this fits into the overall flow. This paper also has a nice table of CUDF-friendly solvers from MISC 2010-11\n\n\n\nThis is an interesting approach if you make a standard - create a competition so others\nuse it!\n\nHow do we evaluate dependency solvers?\n\nExpressivity\n\nA good dependency solver is expressive in its metadata (e.g., allowing me to specify dependencies, conflicts, names, versions) and in it’s client (e.g., I as a user can ask to optimize what I want in my upgrade plan, or maybe even my own criteria)! For BUILD, I’m wondering if we would be extracting binary metadata at runtime, or if it would be done in advance for some set of software (e.g., spack) and then we use it.\n\nComplete\n\nIf there is a solution, the dependency solver finds it.\n\nHow can we summarize current solvers?\n\nThe authors came up with a list of criteria:\n\n\n  versioning scheme: How does the package manager specify versions? (e.g., semvar vs. git strings)\n  distribution: How are packages distributed? GitHub, message pigeons?\n  granularity: What minimal unit can be versioned?\n  version locking: Can developers lock the results of a resolution so itss reproducible?\n  qualifiers: Can we select specific dependencies based on some build config?\n  dependency range operators: This is what it sounds like - using &lt;,&gt;,= to specify versions acceptable\n  range modifiers: Allow developers to select ranges with terms like minor, etc.\n  resolution process: we evaluate based on correctness (are the solutions correct?), completeness (do we always find a solution if one exists?), and user preferences (can I give them?).\n  approximae solutions If a solution isn’t found, can we relax constraints and find one anyway?\n  missing dependencies: If a dependency cannot be satisifed, do we exit and fail, or install the most up-to-date anyway?\n  conflicts: Do we bail with an error if the same package is needed in two versions, or install both of them?\n\n\nAnd here is the table of results from the census, this is a great resource!\n\n\n\nThe authors are happy to report that, based on their cesus, they do see many\npackage managers using more modular, standardized solvers over ad-hoc approaches.\nThey note that only opam has embraced the “separation of concern” idea, and they think this\nis because the CUDF format doesn’t allow for overlapping versions and “dependency constaints involving qualifiers” (I’m not sure what an example of this is). Eg.., opam looks like this:\n\nopam install merlin --criteria=\"-changed,-removed\"\n\nThe social reason that the idea isn’t more widespread is because a lot of major\ntool communities would need to refactor to use a common standard, and one they don’t control,\nand it would be a lot of work.\n\n\n  If it ain’t broken, don’t fix it!\n\n\nI’d also like to point out that this paper got the quote from Field of Dreams incorrect! It’s not:\n\n\n  If you build it, they will come\n\n\nbut actually:\n\n\n  If you build it, he will come\n\n\nHow did the reviewers miss this (ref)?"
					}
					
				
			
		
			
				
					,
					
					"papers-dietrich-msr19": {
						"id": "papers-dietrich-msr19",
						"title": "Dietrich+msr19",
						"categories": "",
						"url": " /papers/dietrich+msr19",
						"content": "Dependency Versioning in the Wild\n\nOverview\n\nThis paper [1] is a nice little study to look at how developers specify versions in the \nwild. They state their four main questions to answer as:\n\n\n  How do projects declare dependencies?\n  Do projects change their approach as they evolve?\n  How do projects adapt semantic versioning?\n\n\nDetails\n\nThey basically started with packages on libraries.io (several millions) and then\nfrom the dependency files (e.g., requirements.txt, pom.xml, etc.) created several regular expressions to match\ndifferent ways to specify versions.  For example, “at least” might have a regular expression to match \n1.2.3,*, and the rest of the classifications are shown in Table 1, below:\n\n\n\nAnd then aggregated classifications futher, e.g.,\n\n\n  SEMVER: matching var-micro or var-minor\n  FLEXIBLE: matching range or soft or any or latest or not or at-least or at-most\n  FIXED: matching fixed\n  OTHER: matching other or unresolved or unclassified\n\n\nUnresolved means that you reached the end of the list of regular expressions and didn’t find a match.\nThey also gave developers a survey asking about how they used semantic versioning (or not)\nand how they generally declared dependencies.\n\nAnd they discovered the following:\n\n\n  Most use some flexible version syntax, and others use semver most predominantly. Maven is an outlier because they think developers probably just copy paste the dependencies from the package websites.\n  Projects get more dependencies as they are developed, with the exception of homebrew. Developers suggest that they move away from a flexible approach to hard code versions because of experiences with bad bugs not doing so (reproducibility!).\n  Most package managers encourage it, only a few require it. It’s checked only superficially.\n\n\nThis makes me want to do a similar project to find a scoped set of packages in a particular package manager,\nand then access the degree of changes per version jump (e.g., lines changed, packages added or removed, etc.).\n\n\n\n\n    J. Dietrich, D. J. Pearce, J. Stringer, A. Tahir, and K. Blincoe, “Dependency Versioning in the Wild,” in Proceedings of the 16th International Conference on Mining Software Repositories, 2019, pp. 349–359.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"papers-hoste-easybuild-pyhpc-2012": {
						"id": "papers-hoste-easybuild-pyhpc-2012",
						"title": "Hoste Easybuild Pyhpc 2012",
						"categories": "",
						"url": " /papers/hoste-easybuild-pyhpc-2012",
						"content": "Easybuild: Building Software with Ease\n\nOverview\n\nEasybuild [1] is similar to spack in that it is a repository of knowledge about how to build\npackages, and is also implemented in Python. It sounds like kenneth Hoste (the creator) first made Easybuild because he realized how\nhard it was to consistently maintain packages. I get the sense (and I think this is confirmed\nby user surveys) that EasyBuild has a larger HPC admin audience, and spack has a larger\n(generic) or non-admin user base.\n\nIssues with Installing Packages\n\nThese are reported issues that inspired creation of EasyBuild at UCGhent:\n\n\n  Incompleteness: Most software requires external libraries (outside the install directory)\n  Non-standard procedure: every install is a custom, gnarly script, or even interactive!\n  custom-built scripts: Instead of make, cmake, etc., packages add their own custom build scripts.\n  Hard coded parameters: are usually in the configuration files / install scripts.\n\n\nThe authors then state that the qualities they would want are:\n\n\n  flexibility: different install procedures should be supported with minimal effort\n  co-existence of versions: so you never need to remove installed software\n  dependency handling: automatic dependency resolution outside of a package manager\n  sharing implementations of install procedures: or basically creating community, reducing redundancy.\n\n\nThe last point advocates for a modular “plug and play” architecture.\n\nUsage\n\nThere is a main command eb that generally works with a main configuration file, and then you\n“point it at” easyconfig files to build packages. It’s fairly nice that it only depends on Python\nand environment modules, although I expect the latter to be a limiting factor in extending to environments\noutside of HPC. It automatically generates an environment module for each package installed, and this is how\nthe same package with different versions can be installed side by side.\n\nGenerally, there are different tools (e.g., easyblocks, extension module) and configuration files\nthat help to specify and then install a package. It’s pretty neat that easyblocks implements sub modules\nas actual packages (e.g., EB_7zip). Basically, they are representing software install procedures in modules\nso they can be shared and extended. Easybuild also has a flietools module to provide wrappers to shell commands,\nand a toolchain package for supporting compilers.\n\nFeatures\n\nOther features of easybuild include keeping track of build logs, archiving successful easyconfig files \nalong with information about EasyBuild version, etc. when it was run, interactive installers,\nparallel installs, regression and unit testing, and “automatic dependency resolution” or looking for easyconfig files that need to be installed.\n\nThe configuration file example basically looks like Python lists, dicts, and strings. The rest\nof the paper has example code and description to do a build.\n\n\n\n\n    K. Hoste, J. Timmerman, A. Georges, and S. D. Weirdt, “EasyBuild: Building Software with Ease,” in 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, 2012, pp. 572–582.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"papers-tuckericse07-opium": {
						"id": "papers-tuckericse07-opium",
						"title": "Tuckericse07 Opium",
						"categories": "",
						"url": " /papers/tuckericse07-opium",
						"content": "OPIUM: Optimal Package Install/Uninstall Manager\n\nOverview\n\nThis [1] is a package manager developed by some guys at UCSD circa 2007. The authors\nclaim that it is complete (if a solution exists, it will be found) and allows for\na new optimization metric to minimize installed packages (via an objective function).\nTheir tool uses Pueblo for the pseudo-boolean solver, the GNU Linear Programming Kit for the ILP solver, and the foci theorum for producing unsatisfiability proofs.\n\nWhy dynamic linking?\n\nStatic linking is when libraries are included alongside a binary (and available immediately) and dynamic linking is when they are found on the host (ref)\n\nAh! This might be how we could parse a binary and figure out it’s dependencies (from the source above):\n\n\n  Every dynamically linked program contains a small, statically linked function that is called when the program starts. This static function only maps the link library into memory and runs the code that the function contains. The link library determines what are all the dynamic libraries which the program requires along with the names of the variables and functions needed from those libraries by reading the information contained in sections of the library.\n\n\nDynamic linking can be more efficient because you just load libraries into memory once.\n\nThe Three Problems of Package Management\n\n\n  The Install Problem can a new package be installed? If so, how?\n  Minimal Install Problem figure out the optimal way to install based on minimizing an objective function.\n  Uninstall Problem: Given a new package to install, figure out the minimal to remove to make it work.\n\n\nThe authors solve each of the problems above as follows:\n\n\n  The Install Problem using a SAT solver on some encoding of the distribution.\n  Minimal Install Problem this encoding is essentially a “pseudo-boolean” problem that solves this issue, something called Pueblo?\n  Uninstall Problem: they tackle this by starting with a package that isn’t installable, and then figuring out what to remove to make it work.\n\n\nTerms\n\n\n  Installation Profile the set of packages installed on a machine. If it’s valid, all dependency clauses (requires, conflicts) are satisfied.\n\n\nThe Install Problem\n\napt-get works by traversing the dependency graph and creating a set of packages that are needed, but it doesn’t go back to look again so it’s less complete. Opium works by creating a variable for each package, and then creating constraints (as equations) for each one that are represented as boolean conditions to evaluate to True or False.\n\nThe Minimal Install Problem\n\nGiven a few sets of solutions, they can assign a cost to things like adding extra packages,\nand then use a SAT solver and bias toward a particular choice (with fewer extras, or minimal size).\n\nThe Uninstall Problem\n\nIf a solution isn’t found, the solver returns a “resolution proof tree” which shows why it wasn’t. The leaves of the tree are packages that can be tested to be removed to see if a solution is then possible.\n\nPages 5-9 have algorithms and proofs for how it works.\n\n\n\n\n    C. Tucker, D. Shuffelton, R. Jhala, and S. Lerner, “OPIUM: Optimal Package Install/Uninstall Manager,” in Proceedings of the 29th International Conference on Software Engineering, USA, 2007, pp. 178–188.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"papers-vieiraase02": {
						"id": "papers-vieiraase02",
						"title": "Vieiraase02",
						"categories": "",
						"url": " /papers/vieiraase02",
						"content": "Analyzing Dependencies in Large Component-Based Systems\n\nOverview\n\nThe authors [1] introduce component-based systems as a good practice to develop software,\nbut say that enough work hasn’t been done to understand dependency management.\nThe authors are suggesting an approach to model dependencies in… XML scream\n(looks that paper is from 2002, phew) called CBDM, or the Component Based Dependence Model. \nI don’t think this model took over, so this might be an example of what not to do.\n\nComponent Based Systems\n\nOr CBS is a collection od dependencies that rise from some original set of components.\nLogically, tracking components gets harder as you install more things. This is the first\npaper that recognizes two kinds of components - “in house” (stuff you make) and\n“third party” (what others make). The authors suggest third party components are more akin\nto black boxes.\n\nOverview of Component Dependencies\n\nSome CBS have components “bundled in” but typically they depend on:\n\n\n  services from other components\n  functionalities from external resources (e.g., middleware or OS)\n\n\nAnd also have internal dependencies built in that can impact inputs and outputs.\nThe authors seem to be making the argument that because it’s hard to know this set\nof dependencies, they need their XML schema.\n\nThe Approach Proposed\n\nThe authors want to create this XML schema to:\n\n\n  describe the dependencies\n  analyze influence\n\n\nAnd it should be flexible enough to:\n\n\n  be descriptive\n  define things precisely\n  be easy to change when new information is available\n\n\nand the result is the Component Based Dependence Model (CBDM) which is XML based,\nand broken into two parts: internal and external (hardware, software and services, and causality, provided and required) dependencies. It’s basically a dependency graph of components and services. The process of analyzing dependencies takes the following 3 steps:\n\n\n  figure out what components are on the system (and configuration)\n  create the CBDM\n  identify component dependencies in the CBS.\n\n\nThey chose XML to be able to query it like a graph, describe pomsets (multi-level, e.g., runtime dependencies), and still express things in text. I just did a Google search and only turned up this paper as a result, so I’m guessing it didn’t stand the test of time.\n\npomsets\n\nAccording to the authors, a pomset consists of a sequence of actions in the form of a regex,\nwhere each action could be:\n\n\n  describing services not provided by the component\n  actions that the component can provide via some interface\n  actions related to some internal service (not provided by the component)\n\n\nThe authors also state that providing a complete set of metadata (the pomsets) is a limitation because it’s likely arduous (and maybe this is one reason why this approach\ndidn’t carry forward). It would be really nice to see an example of this, the description is fairly vague.\n\n\n\n\n    M. Vieira and D. Richardson, “Analyzing dependencies in large component-based systems,” in Proceedings 17th IEEE International Conference on Automated Software Engineering, 2002, pp. 241–244.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"papers-wiese2019naming": {
						"id": "papers-wiese2019naming",
						"title": "Wiese2019naming",
						"categories": "",
						"url": " /papers/wiese2019naming",
						"content": "Naming the Pain in Developing Scientific Software\n\nOverview\n\nThe paper [1] strongly reflects the state of research software engineering - a large number of us don’t have official/extensive training in computer science or software engineering, and thus this leads to what the author is calling “pains.” This paper is cool because they attempt to characterize these pains, namely building a taxonomy of 2,110 problems across 1,577 R developers. Areas of pain include:\n\n\n  technical-related\n  social-related\n  scientific-related\n\n\nAnd developing research software is hard because:\n\n\n  requirements constantly chang\n  RSEs need domain knowledge and programming expertise\n  architectures (e.g., communication) change frequently\n  tooling is often not open source, or not community efforts (and I’d guess everyone re-invents the wheel).\n\n\nAnd the primary complaints they found are:\n\n\n  lack of documentation\n  poor compatability across platforms\n  lack of a formal reward system\n\n\nWho is the Scientific Software Developer?\n\nThey tried to filter (by asking) to only developers of scientific software.\n\n\n  Most are male (88%)\n  It’s fairly globally diverse, with most in Europe (49%) and the US (34%)\n  80% are working toward or have a PhD\n  Only 20% call themselves software engineers\n  fields / domains are pretty much everything\n\n\nThe Pains\n\nTechnical Related\n\nTechnical problems are over 70% of reported issues. Top issues include:\n\n\n  software requirements and management\n  functional requirements\n  documentation\n  software testing/debugging\n\n\n\n\nand of course you can see the entire breakdown in the figure above!\nThey think that this category is high possibly because there are fewer\nrespondents with CS backgrounds (3%).\n\nSocial Problems\n\nTop social problems include:\n\n\n  Lack of Time\n  Publicity (e.g., we need journals like JOSS)\n  Emotional aspects (no recognition sucks)\n  Communication and Collaboration (lack of scientific software community)\n  Lack of Support (e.g., smaller user base == less feedback)\n\n\n\n\nScientific Related Problems\n\n\n  Data handling pains (poor community standards, data quality, management, privacy, sharing)\n  reproducibility (people still don’t care sometimes)\n  scope of software (often unclear)\n  background (mapping between domain and coding skills)\n\n\n\n\nSolutions\n\nThe authors state that solutions aren’t the goal of the paper, but they point out that Software carpentry encourages:\n\n\n  using version control\n  turning bugs into test cases\n  consistent style/formatting of code\n\n\nThese, however, are mostly technical and don’t address the social / scientific problems\nmentioned previously. The authors say that based on their findings, past work doesn’t properly quantify and then address these “softer” problems, and they anticipate more research to come (see Table 1 at end of paper).\n\nDatasets\n\n\n  The research data is available at https://github.com/utfpr/NamingThePain.\n\n\n\n\n\n    I. S. Wiese, I. Polato, and G. Pinto, “Naming the Pain in Developing Scientific Software,” IEEE Software, 2019.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"papers-wintersewg20-now-or-never": {
						"id": "papers-wintersewg20-now-or-never",
						"title": "Wintersewg20 Now Or Never",
						"categories": "",
						"url": " /papers/wintersewg20-now-or-never",
						"content": "What is ABI, and What Should WG21 Do About It?\n\nTLDR\n\nABI == Application Binary Interface. This (document? email?) seems to be talking\nabout it in context of WG21, which I can only guess means “Working Group 2021,” some\nkind of group that reviews changes to C++ that might break ABI (and thus doesn’t allow).\nThis author, however is suggesting that:\n\nWe should change themangling for C++23 symbols in a way that is not compatible with previous C++manglings.​[1]\n\nand changing the manglings so it can be detected at load time via a prefix (e.g., _Z vs _Y). He thinks it’s reasonable to break ABI every ~3 years, and force rebuilds. He thinks \nthe other alternative (not breaking it) will lose industry involvement. It’s interesting that this seems to be a cultural thing - so far the standard has been to not break it,\nbut there are greater costs to this over time.\n\nOverview\n\nThe idea of ABI compatibility is much simpler than I anticipated. This might be an oversimplifcation, but when we compile the code in C/C++, we have “mangled strings”\nthat are left behind in the binary to help with linking (this could be more specific).\nA nice example from Wikipedia is the following:\n\nnamespace​ wikipedia {\n    ​class​ article ​{\n        public​:​\n             /* Mangles as: _ZN9wikipedia7article6formatEv */      \n             std​::​string​ format​(​void​);\n         ​};\n    };\n\n\nSo if we change the name, arguments, or even template arguments for a function,\nthat is an ABI change because it changes this “mangled name.”\n\nAn ABI Break is thus when something changes to warrant using a different function\nto generate the mangled name, or generating a different mangled name all together.\nIf two compiled binaries are talking to one another, they have to agree on that name.\nThe document mentions that it’s no different than a binary file format or network protocol\nin that sender and receiver have to agree on how to interpret a file.\n\nHyrum’s Law says that when you have enough users of an API, all endpoints/behaviors\nare going to be depended on by somebody. This is an argument for not breaking ABI because so many depend on it. E.g., the ecosystem (gcc and clang) has used it for 10+ years and changing it would break a lot. But it’s also an argument for breaking ABI because it’s inevitable.\n\nThe author is worried that if we don’t break it, industry involvement will decrease,\nbecause not breaking it means too many sacrifices to performance.\n\nWG21\n\nThe purpose of this group is revealed on page 5:\n\n\n  …the behavior of WG21 for several years has been to give standardlibrary implementers an effective veto on any proposal that would break ABI.\n\n\nWow, so we need humans to check and manage these changes. Isn’t that error prone? \nThe author mentions taking an “ABI break” and I’m not clear what that means.\nWouldn’t so many things break?\n\n\n\n\n    T. Winters, “What is ABI, and What Should WG21Do About It?,” in C++ Standards Committee WG21, 2020.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"papers-wintersewg20-what-is-abi": {
						"id": "papers-wintersewg20-what-is-abi",
						"title": "Wintersewg20 What Is Abi",
						"categories": "",
						"url": " /papers/wintersewg20-what-is-abi",
						"content": "Now or Never\n\nOverview\n\nThis paper [1] appears to be a document similar to [2],\nThe author opens by saying for C++14,17,20 have been ABI stable, and he’s been advocating to break it, but is changing his mind. He brings up culture and user expectations - people\nexpect ABI to be stable, and he doesn’t think the list of changes (that he estimates would improve performance 5-10%) that are warranted outweigh\nthe damage to the ecosystem that would result. He estimates the costs to be “an engineer-millenia.”\n\nThe author is saying that if ABI is not to be broken, the WG21 should be very\n“public about that decision” instead of wishy washy. Conversely, if they decide to break ABI, they need to be principled about it. The author summarizes three options he sees:\n\n\n  Decide on when to break ABI (e.g., release C++23/26) and provide a heads up and diagnostic tools.\n  Decide not to break it, and commit to stability.\n  Do nothing, and stay like we are now (wishy washy, at best).\n\n\nHe adds subtext that 2 is boring and safe, and 1 is best for users that want to maximize performance. 3 is really a failure of leadership. He is worried that if we don’t break ABI, we leave room for a competitor, more performant language to be created.\n\n\n\n\n    T. Winters, “ABI - Now or Never,” in C++ Standards Committee WG21, 2020.\n    Full details |\n    PDF\n    \n\n\n\n\n    T. Winters, “What is ABI, and What Should WG21Do About It?,” in C++ Standards Committee WG21, 2020.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"papers-yurichev2018sat": {
						"id": "papers-yurichev2018sat",
						"title": "Yurichev2018sat",
						"categories": "",
						"url": " /papers/yurichev2018sat",
						"content": "SAT/SMT by Example\n\nOverview\n\nThis [1] is an entire book on solvers! Despite the name, the document appears to be dated in 2020.\n\n\n  SMT Solvers take systems in arbitrary format. The book (starting on page 13) gives many applied examples of using software to solve a system of equations (even XKCD!) and mentions that SMT solvers are really the front-ends to SAT solvers. E.g., we take some boolean logic, do one-hot encoding (called “bit blasting), and get a SAT solver problem.\n  SAT solvers are limited to boolean equations in CNF form\n  CNF form is basically a bunch of sums, and’s and or’s (definition) or what I’d think of as boolean logic.\n\n\nThis is a long book, so I’m not decided how to best take notes yet, possibly it’s best to use as a reference.\n\n\n\n\n    D. Yurichev, “SAT/SMT by example.” Online.\n    Full details |\n    PDF"
					}
					
				
			
		
		
			
				
					,
					
					"notes-containers": {
						"id": "notes-containers",
						"title": "Containers Investigation for Build",
						"categories": "containers",
						"url": " /notes/containers",
						"content": "tocIn this post\n  \n  \n\n  Overview\n  Proposed R&amp;D Tasks    \n      1. Understand models of container ABI compatibility\n      2. Research ways to dynamically assemble software stacks around ABI constraints.\n      3. What are the inherent complexities around integrating modern software stacks with HPC?\n      4. How can we build infrastructure to support ABI-based dependency resolution?\n    \n  \n  Ideas for Projects    \n      Decision Comparison\n      Bootstrap CUDF\n    \n  \n\n\n  \n\n\nOverview\n\nOne of the particularly hard aspects of compatibility that we plan to investigate under BUILD is the relationship between the binaries in containers with the binaries on the host system.  Containers in the cloud encapsulate a software ecosystem, generally for a single application and all of its dependencies for some base operating system.  Used as initially intended, containers enable very complex applications to be used portably across machines. However, in HPC, the software ecosystem cannot be fully encapsulated as libraries like MPI, GPU runtimes, and libfabrics typically require access to hardware and drivers on the host OS.  There are subtleties around how best to bindmount libraries and other software from the host OS into a container while ensuring compatibility.  In most cases today, compatibility is not ensured, and containers are simply run until they fail (due to an OS update or some other incompatibility).\n\nBUILD aims to do better by doing detailed comparison of host and containerized software stacks and selecting the best interfaces to use for maximum container portability and performance.  These types of interfaces are particularly important for machine learning applications that are a) hard to build and b) require good performance to run. We will investigate how best to package HPC and ML applications in containers and how to select, validate, and maintain a good binary interface between container and host OS’s.\n\nProposed R&amp;D Tasks\nProposed research and development topics include (but are not limited to) the following:\n\n1. Understand models of container ABI compatibility\nThis can include MPI libraries, GPU libraries, libfabrics, scheduler integration, or any interface that needs to remain compatible to ensure container portability. We will conduct studies to determine rules around ABI layer selection, analysis techniques to compare ABI compatibility between containers and hosts, and best practices for maintaining compatibility over time.\n\n2. Research ways to dynamically assemble software stacks around ABI constraints.\nRunning a container with hardware support on a host OS requires us to marry two or more independently curated software ecosystems (container and run host(s)) together. Given a set of ABI constraints, e.g. from several base OS or library versions, can we build a container that satisfies all of them? Can two applications be consolidated based on ABI requirements, or do they need to be in separate containers?How can we construct maximally performant and portable software stacks?\n\n3. What are the inherent complexities around integrating modern software stacks with HPC?\nModern open source software stacks evolve very rapidly, and it can be very hard for HPC applications to keep up with this rate of change. Often, rapid evolution comes at a price – these stacks are targeted at very specific environments, and adapting them for others (like HPC) can be difficult. For example, TensorFlow, a very popular machine learning tool, is maintained specifically for Google’s internal machines, and can be very difficult to build for HPC systems. Other language ecosystems like Python, Ruby, R, and JavaScript can also be hard to adapt for maximum performance.  How can we derive HPC-compatible builds of tools like these? How can we leverage host HPC software stacks from within portably built containers, and how can we guarantee portability over time?\n\n4. How can we build infrastructure to support ABI-based dependency resolution?\nBUILD aims to build dependency resolvers that check ABI and solve around ABI conflicts automatically.  There will likely need to be databases of ABI and build information to back such a resolver, and we have not, so far, investigated how best to build these. We will leverage Dr. Sochat’s expertise both in software ecosystems, containers, and scientific workflow design to build such infrastructure.\n\n\n\nIdeas for Projects\n\nStill unrelated to the R&amp;D tasks above, the following projects are inspired by papers, and could be useful\nin some way integrated into the tasks above.\n\nDecision Comparison\n\nIf we need to (superficially) compare how package managers make decisions, we could select a small number of packages,\nand akin to how they did in [abate-2013-modular-package-manager]\n attempt to install\neach package across different package managers (e.g., apt-get, yum, aptitute, etc.) and parse the output to determine how\nthe managers make decisions [1]. We would then want to look at how each one works and map the behavior that we see to \nsome solver or heuristic.\n\nThis would help to understand how well-known package managers work, and to familiarize with them so that\nif we wanted to imagine changing them to be more modular, we might have a sense of how to go about it.\n\nBootstrap CUDF\n\nI was reading in [1] that CUDF can allow for definition of “additional properties.”\nWould our first efforts want to try and define additional properties related to ABI compatability, and just use standard CUDF? I realize now that I don’t have a sense of if CUDF is a good idea/direction or not. Based on the papers presenting it, it seems to be?\n\n\n\n\n    P. Abate, R. Di Cosmo, R. Treinen, and S. Zacchiroli, http://www.sciencedirect.com/science/article/pii/S0950584912001851“A modular package manager architecture”&lt;/a&gt; Information and Software Technology, vol. 55, no. 2, pp. 459–474, 2013.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"notes-find-mangled-strings": {
						"id": "notes-find-mangled-strings",
						"title": "Finding Mangled Strings",
						"categories": "binary",
						"url": " /notes/find-mangled-strings",
						"content": "tocIn this post\n  \n  \n\n  \n\n\nI’ve been reading about these “mangled strings” that are left behind by compilers as a hint\nto the functions that are needed, and I wanted to give a shot at actually seeing them.\nBased on this thread I’m deriving my own example. First, let’s start with a file\nto compile:\n\n#include &lt;stdio.h&gt;\n\n\nclass Car {\npublic:\n\tCar() { }\n\t\n\tvirtual void Honk() = 0;\n};\n\n\nclass RedCar : public Car {\npublic:\n\tRedCar() { }\n\t\n\tvirtual void Honk();\n};\n\n\nvoid RedCar::Honk() {\n\tprintf( \"Honk!\\n\" );\n}\n\n\nint main( int argc, char *argv[] ) {\n\tRedCar mycar;\n\tmycar.Honk();\n\treturn 0;\n}\n\n\nWe then can create a Makefile that handles:\n\n\n  compiling the cpp file\n  linking\n  stripping\n\n\nCC = g++\nCFLAGS = -Wall -fno-rtti -Os -fdata-sections -ffunction-sections # -Wall -O2\nTARGET = car\n \nall: $(TARGET).stripped\n\n$(TARGET).stripped: $(TARGET)\n\tstrip -o $(TARGET).stripped $(TARGET)\n\n$(TARGET): $(TARGET).o\n\t$(CC) -Wl,-O,-s,--gc-sections $(TARGET).o -o $(TARGET)\n \n$(TARGET).o: $(TARGET).cpp\n\t$(CC) $(CFLAGS) -c $(TARGET).cpp -o $(TARGET).o\n\nclean:\n\t$(RM) $(TARGET) $(TARGET).stripped $(TARGET).o\n\n\nAnd then for the final stripped output, we can run:\n\n$ strings car.stripped \n/lib64/ld-linux-x86-64.so.2\nlibc.so.6\nputs\n__stack_chk_fail\n__cxa_finalize\n__libc_start_main\nGLIBC_2.4\nGLIBC_2.2.5\n_ITM_deregisterTMCloneTable\n__gmon_start__\n_ITM_registerTMCloneTable\n5z\t \n=1\t \nAWAVI\nAUATL\n[]A\\A]A^A_\nHonk!\n;*3$\"\nGCC: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n.shstrtab\n.interp\n.note.ABI-tag\n.note.gnu.build-id\n.gnu.hash\n.dynsym\n.dynstr\n.gnu.version\n.gnu.version_r\n.rela.dyn\n.rela.plt\n.init\n.plt.got\n.text\n.fini\n.rodata\n.eh_frame_hdr\n.eh_frame\n.init_array\n.fini_array\n.data.rel.ro\n.dynamic\n.data\n.bss\n\n\nThis is getting us somewhere! In the above we see what looks like metadata, along\nwith library versions, compilers, and function names. The forum mentions using something\ncalled “sstrip” for a more aggressive stripping, but it doesn’t seem to be installed by default.\nThese files can be found under the examples folder if you want to try them out. Please submit a PR if you want to extend this example, or give better context."
					}
					
				
			
		
			
				
					,
					
					"notes-flow": {
						"id": "notes-flow",
						"title": "Package Manager Flow",
						"categories": "",
						"url": " /notes/flow",
						"content": "tocIn this post\n  \n  \n\n  Overview\n  Software Maintenance Technologies\n  Package Manager\n  User preferences\n  Scenarios\n  Requests\n  Terms\n  Solvers\n\n\n  \n\n\nOverview\n\nWe should eventually be able to describe the makeup of a package manager - akin to how the\nOpen Containers Initiative has an image-spec, distribution-spec, and runtime-spec for containers,\nwe want similar specs to describe actions and states for package management. I don’t have a holistic view yet\nof what this should look like, but I’ll include interesting notes that are relevant.\nHere is an early understanding of a general flow:\n\nuser preferences --&gt; package manager --&gt; translate to upgrade problem (CUDF) --&gt; solver --&gt;  --&gt; CUDF-encoded solution --&gt; scenario --&gt; [if solution] --&gt; action [else] rollback\n\n\nThe above says that we start with user preferences that are assembled by the package manager, hand them off to a solver (package manager agnostic, used like plugins) to come up\nwith a solution. There are several ways to describe a solution (scenario) that determines\nif we move forward or not. The input and output to the solver is a CUDF document that describes the needed changes. There are also terms that can describe the scenarios terms\n\nSoftware Maintenance Technologies\n\nThe “ingredients” of “software maintenance technologies” according to [1] are:\n\n\n  dependencies\n  conflicts\n  package managers with dependency solving capabilities\n\n\nPackage Manager\n\nThe package manager should generally have the following inputs and outputs [2]\n\n\n  Take as input:\n    \n      the current status of packages on the system\n      a universe of all available packages (in other papers called U)\n      a user request (e.g., “install xyz, remove xyz”)\n      user preferences (e.g., “minimize updated software”)\n    \n  \n  Return as output:\n    \n      an upgrade plan: a list of actions to take on the system to reach the status that the user wants (e.g., installing thething)\n    \n  \n\n\nAnd we can evaluate these steps based on expressivity - how empowered the user is to\nexpress preferences, and completeness - being able to propose a valid update plan (the output) when one exists.\n\nIn [2] they (mostly same) authors make a similar statement, saying that package managers should (this is verbatim):\n\n\n  devise upgrade plans that are correct (i.e. no plan that violates component expectations is proposed) and complete (i.e. every time asuitable plan exists, it can be found)\n  have performances that scale up gracefully at component repositories growth\n  empower users to express preferences on the desired component configuration when several options exist, which is often the case\n\n\nUser preferences\n\nThe user should be able to specify high level criteria to describe what they want (e.g., minimize changed packages).\nOf course the more criteria we define, the more likely we are to not be able to find a solution or\nhave something pareto-optimal. Approaches to deal with this include:\n\n\n  Lexicographic: order criteria by importance (e.g., a security update gets ranked higher)\n  Weighted sum: aggregate criteria into single measure using some set of weights\n  lexmin and lexmax: (haven’t read about yet)\n\n\nThe authors in [3, P. 14] propose to do the following:\n\n\n  define a dictionary of criteria\n  define a dictionary of aggregation functions (e.g., the above)\n  write the user preference as an expression of the aggregation function (op) and directionality (e.g., +/-) for each criteria. E.g.,\n\n\nlex(-removed, -changed)\n\n\nSays use lexmin to aggregate, and minimize changed and removed packages. They present a nice table of\noptimization criteria:\n\n\n\nAnd this is a neat idea because you can then describe specific scenarios (and name them)\n\nparanoid=lex(−removed,−changed)\ntrendy=lex(−removed,−notuptodate,−unsatrec,−new)\n\n\nScenarios\n\nThe paper [3, P. 12] describes different solution scenarios,\nrooted in CUDF, and this would be a good terminology to harden to better describe our work.\n\nWe start with a CUDF document\n and define a solution to it \nas a subset (S) of the entire package universe (U). We then say that dom(S) (domain) are the pairs\nof packages (e.g., (name, version) in our subset (e.g., what CUDF is asking for) in  S, and pro(S) (provided)\nthe same (name, version) pairs of all that are available, which can be infinite (gulp!).\n\nThis defines an atomic package constraint. A subset S in U is (these are written verbatim from the paper):\n\n\n  abundant if every disjunction in the dependency of every package in S contains a package constraint that is satisfied by dom(S) ∪ pro(S). I think this is saying that a solution is abundant if every nested dependency has it’s own dependency needs met by either the set of pairs that we need, or the set of pairs available to us. It’s abundant possibly because it says that the universe is rich with everything that we need.\n  peaceful if no atomic package constraint in the conflicts of any package p∈S is satisfied by dom(S−{p}) ∪ pro(S−{p}). I think this is saying that if we removed p from both sets, there would be no conflict.\n\n\nRequests\n\nA subest S of the universe U satisfies a request:\n\n\n  install φ if every atomic constraint in φ is satisfied by dom(S) ∪ pro(S). This is saying we move forward to install if the universe is abundant?)\n  remove φ if no element of φ is satisfied by dom(S) ∪ pro(S).\n\n\nA set S is only a solution if it is abundant, healthy, and satisfies the request.\nSolutions are allowed to have several packages with the same name and different versions.\n\nTerms\n\nWe can further classify solutions based on:\n\n\n  correctness: have we satisfied the user request?\n  quality: do I like the solution?\n\n\nSolvers\n\nIt looks like there are many solver technologies, as references by [3] in the MISC competition (Mancoosi International Solver Competition ref) including but not limited to:\n\n\n  boolean satisfiability (SAT)\n  Mixed Integer Linear Programming (MILP)\n  Answer Set Programming (ASP)\n  graph constraints\n\n\nAnd I think we intend to work on one based on ABI compatability. Many of these can handle upgrade problems encoded as CUDF documents.\n\n\n\n    P. Abate, R. Di Cosmo, R. Treinen, and S. Zacchiroli, “Dependency solving: a separate concern in component evolution management,” Journal of Systems and Software, vol. 85, no. 10, pp. 2228–2240, 2012.\n    Full details |\n    PDF\n    \n\n\n\n\n    P. Abate, R. Di Cosmo, G. Gousios, and S. Zacchiroli, “Dependency Solving Is Still Hard, but We Are Getting Better at It,” in 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER), 2020, pp. 547–551.\n    Full details |\n    PDF\n    \n\n\n\n\n    P. Abate, R. Di Cosmo, R. Treinen, and S. Zacchiroli, http://www.sciencedirect.com/science/article/pii/S0950584912001851“A modular package manager architecture”&lt;/a&gt; Information and Software Technology, vol. 55, no. 2, pp. 459–474, 2013.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"notes-projects": {
						"id": "notes-projects",
						"title": "Projects",
						"categories": "projects",
						"url": " /notes/projects",
						"content": "tocIn this post\n  \n  \n\n  Projects    \n      ABI compatability\n    \n  \n\n\n  \n\n\nProjects\n\nThe following are mini-projects that I’d like to try:\n\nABI compatability\n\nIf ABI compatability comes down to mangled strings (e.g., [1][2]) then I’d want to:\n\n\n  Figure out how to parse a binary to collect all “mangled strings”\n  Build a container that can install a library across multiple versions\n  Inspect how the strings change (if at all)\n  Perhaps create a tool that can easily do this.\n\n\nI think this would be a start to figuring out ABI compatability, because we could\nthen easily parse two things and calculate some similarity score between them based on\nthe strings! I don’t know if this is the right way to go about it, but it seems like it would\nbe fun to do.\n\n\n\n\n    T. Winters, “ABI - Now or Never,” in C++ Standards Committee WG21, 2020.\n    Full details |\n    PDF\n    \n\n\n\n\n    T. Winters, “What is ABI, and What Should WG21Do About It?,” in C++ Standards Committee WG21, 2020.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"notes-questions": {
						"id": "notes-questions",
						"title": "Questions",
						"categories": "questions",
						"url": " /notes/questions",
						"content": "tocIn this post\n  \n  \n\n  Questions    \n      Are there alternatives to package managers?\n      Are more constraints better?\n      Will there ever not be ABI?\n    \n  \n\n\n  \n\n\nQuestions\n\nAre there alternatives to package managers?\n\nIf a lot of the complexity comes down to the upgrade problem (needing to install a new package given an existing set, and then upgrade some subset of that) we could arguably just maintain one container per package, each with the exact dependencies needed by that package.\nThis means that when we want a new package version, we throw away the old container and build/get a new one.\n\nI’m not saying this is a good solution (it’s very redundant) but it’s inspiring for thought!\n\nAre more constraints better?\n\n[1] mentioned that it gets more complex as the size of the repository increases and you add more metadata, but it seems like being able to eliminate solutions more quickly that don’t fit with some architecture would get us faster to a solution (or a no solution). So is the trick having fewer metadata points that are more useful (e.g., something derived from the package binaries is more meaningful than random user specifications of versions needed?)\n\nWill there ever not be ABI?\n\nThe paper (missing reference) says:\n\nFor the past few years, I have been keeping an informal list of things to clean up in the standardlibrary if and when we take an ABI break.(missing reference)\n\nWhat exactly is an “ABI break” and does it suggest a world where this isn’t relevant anymore? Would a solver that is based on ABI compatibiltiy make sense in such a world?\n\n\n    P. Abate, R. Di Cosmo, R. Treinen, and S. Zacchiroli, “Dependency solving: a separate concern in component evolution management,” Journal of Systems and Software, vol. 85, no. 10, pp. 2228–2240, 2012.\n    Full details |\n    PDF"
					}
					
				
			
		
			
				
					,
					
					"notes-thrusts-thrust1": {
						"id": "notes-thrusts-thrust1",
						"title": "Thrust 1: Develop a formal specification to determine compatibility",
						"categories": "",
						"url": " /notes/thrusts/thrust1",
						"content": "Develop a formal specification to determine compatibility, focusing on Application Binary Interfaces (ABI) between packages. This includes the development of a tool to programmatically construct and verify compatibility models and a set of use cases from proxy apps that demonstrate mixed language/runtime applications can be built and verified."
					}
					
				
			
		
			
				
					,
					
					"notes-thrusts-thrust2": {
						"id": "notes-thrusts-thrust2",
						"title": "Thrust 2: Develop binary analysis tools to determine package integration",
						"categories": "",
						"url": " /notes/thrusts/thrust2",
						"content": "Develop binary analysis tools to determine which packages can be integrated together, even when built through different means (e.g., manually or with a package manager). This requires reverse engineering stripped binaries with no debug information to find entry/exit points, exported data types/structures, other ABI relevant semantics and metadata, and validating equivalence or near equivalence in function behavior."
					}
					
				
			
		
			
				
					,
					
					"notes-thrusts-thrust3": {
						"id": "notes-thrusts-thrust3",
						"title": "Thrust 3: Quickly find the right combination of versions to satisfy constraints",
						"categories": "",
						"url": " /notes/thrusts/thrust3",
						"content": "Quickly find the right combination of versions to satisfy constraints. This thrust builds on top of verifiable compatibility models and binary analysis. For this area, the team wants to determine if modern solvers can be used to solve full-scale models quickly enough to be practical and what types of encodings and heuristics are necessary to reliably solve this problem."
					}
					
				
			
		
			
				
					,
					
					"notes-thrusts-thrust4": {
						"id": "notes-thrusts-thrust4",
						"title": "Thrust 4: Find the optimal configuration",
						"categories": "",
						"url": " /notes/thrusts/thrust4",
						"content": "Find the optimal configuration that is closest to known and tested versions of the code. They want to leverage the large set of library configurations (~4,000 spack packages) to train a machine learning model to find optimal configurations."
					}
					
				
			
		
	};
</script>

		</div>
	</div>
</main>

	<script src="/build-notes/assets/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript">
  jQuery(document).ready(function($){
    // browser window scroll (in pixels) after which the "back to top" link is shown
    var offset = 300,
      //browser window scroll (in pixels) after which the "back to top" link opacity is reduced
      offset_opacity = 1200,
      //duration of the top scrolling animation (in ms)
      scroll_top_duration = 700,
      //grab the "back to top" link
      $back_to_top = $('.cd-top');

    //hide or show the "back to top" link
    $(window).scroll(function(){
      ( $(this).scrollTop() > offset ) ? $back_to_top.addClass('cd-is-visible') : $back_to_top.removeClass('cd-is-visible cd-fade-out');
      // if( $(this).scrollTop() > offset_opacity ) { 
      //  $back_to_top.addClass('cd-fade-out');
      // }
    });

    //smooth scroll to top
    $back_to_top.on('click', function(event){
      event.preventDefault();
      $('body,html').animate({
        scrollTop: 0 ,
        }, scroll_top_duration
      );
    });

  });
</script>
<style type="text/css">
.cd-top {
  display: inline-block;
  height: 50px;
  width: 50px;
  position: fixed;
  bottom: 2%;
  right: 2%;
  border-radius: 40px;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.05);
  /* image replacement properties */
  overflow: hidden;
  text-indent: 100%;
  white-space: nowrap;
  background: #bbb url(/build-notes/assets/img/cd-top-arrow.svg) no-repeat center 50%;
  visibility: hidden;
  opacity: 0;
  -webkit-transition: opacity .3s 0s, visibility 0s .3s;
  -moz-transition: opacity .3s 0s, visibility 0s .3s;
  transition: opacity .3s 0s, visibility 0s .3s;
}
.cd-top.cd-is-visible, .cd-top.cd-fade-out, .no-touch .cd-top:hover {
  -webkit-transition: opacity .3s 0s, visibility 0s 0s;
  -moz-transition: opacity .3s 0s, visibility 0s 0s;
  transition: opacity .3s 0s, visibility 0s 0s;
}
.cd-top.cd-is-visible {
  /* the button becomes visible */
  visibility: visible;
  opacity: 1;
}
.cd-top.cd-fade-out {
  /* if the user keeps scrolling down, the button is out of focus and becomes less visible */
  opacity: .5;
}
.no-touch .cd-top:hover {
  background-color: #e86256;
  opacity: 1;
}
</style>

<a href="#0" class="cd-top">Top</a>

	<footer class="page-footer teal">
  <div class="footer-copyright">
    <div class="container text-white">
          <a class="waves-effect" href="/build-notes/tags">Tags</a> | <a class="waves-effect" href="/build-notes/categories">Categories</a> | <a class="waves-effect" href="https://github.com/buildsi/build-notes">GitHub</a>
    </div>
  </div>
</footer>

<script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/js/materialize.min.js"></script>




  
    <script src="/build-notes/assets/js/lunr.min.js"></script>
  

  
    <script src="/build-notes/assets/js/search.js"></script>
  

<script src="/build-notes/assets/js/main.js"></script>

</body>
</html>